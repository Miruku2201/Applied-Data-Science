{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Crawling data from \"vietnamworks\" webpage__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Dữ liệu \"job requirement\" được lấy từ 4 trang liên tục, và được lưu ở thư mục `vietnamworks/data` theo ngày được lấy.\n",
    "+ Vì chưa có function cho việc crawl dữ liệu nên có thể thay đổi các thông số thích hợp trong file `get_job_url.py`. Cần thay đổi giá trị biến `NUMBER_PAGES_LOADING` để thay đổi số trang cần crawl.\n",
    "+ Import file `get_job_url_ để run code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payload:  1\n",
      "payload:  2\n",
      "payload:  3\n",
      "payload:  4\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import get_job_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Tiến hành crawl dữ liệu__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Truy cập đường dẫn thư mục data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d:\\\\Coding\\\\HCMUS\\\\DSa\\\\BrowserCrawler\\\\vietnamworks', 'c:\\\\Users\\\\PC\\\\anaconda3\\\\envs\\\\DSAIenv\\\\python311.zip', 'c:\\\\Users\\\\PC\\\\anaconda3\\\\envs\\\\DSAIenv\\\\DLLs', 'c:\\\\Users\\\\PC\\\\anaconda3\\\\envs\\\\DSAIenv\\\\Lib', 'c:\\\\Users\\\\PC\\\\anaconda3\\\\envs\\\\DSAIenv', '', 'c:\\\\Users\\\\PC\\\\anaconda3\\\\envs\\\\DSAIenv\\\\Lib\\\\site-packages', 'c:\\\\Users\\\\PC\\\\anaconda3\\\\envs\\\\DSAIenv\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\PC\\\\anaconda3\\\\envs\\\\DSAIenv\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\PC\\\\anaconda3\\\\envs\\\\DSAIenv\\\\Lib\\\\site-packages\\\\Pythonwin', 'vietnamworks/data', 'data']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('vietnamworks/data')\n",
    "sys.path.append('data')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Import file `urls_not_exist` để tìm ra các urls chưa tồn tại trong cơ sở dữ liệu, và lưu vào trong data bên trong folder `crawl_info` do thư viện `scrapy` quản lý."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oke\n"
     ]
    }
   ],
   "source": [
    "import urls_not_exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Tiến hành crawl dữ liệu vừa thu thập được. Dữ liệu vừa thu thập được sẽ được ghi vào file `out.json` trong thư mục `crawl_info/crawl_info/json/out.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !scrapy crawl vnw_i4_crawler -o out.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Coding\\HCMUS\\DSa\\BrowserCrawler\\vietnamworks\\crawl_info\\crawl_info\n"
     ]
    }
   ],
   "source": [
    "cd crawl_info/crawl_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRORS: 0\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 08:26:02 [scrapy.utils.log] INFO: Scrapy 2.11.1 started (bot: crawl_info)\n",
      "2024-04-08 08:26:02 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 23.10.0, Python 3.11.8 | packaged by Anaconda, Inc. | (main, Feb 26 2024, 21:34:05) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 24.0.0 (OpenSSL 3.2.1 30 Jan 2024), cryptography 42.0.5, Platform Windows-10-10.0.22631-SP0\n",
      "2024-04-08 08:26:02 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-04-08 08:26:02 [asyncio] DEBUG: Using selector: SelectSelector\n",
      "2024-04-08 08:26:02 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
      "2024-04-08 08:26:02 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop\n",
      "2024-04-08 08:26:02 [scrapy.extensions.telnet] INFO: Telnet Password: 2e2fbe532cf3a9d9\n",
      "2024-04-08 08:26:02 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-04-08 08:26:02 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'crawl_info',\n",
      " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
      " 'NEWSPIDER_MODULE': 'crawl_info.spiders',\n",
      " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
      " 'SPIDER_MODULES': ['crawl_info.spiders'],\n",
      " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',\n",
      " 'USER_AGENT': 'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) '\n",
      "               'AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148'}\n",
      "2024-04-08 08:26:02 [faker.factory] DEBUG: Not in REPL -> leaving logger event level as is.\n",
      "2024-04-08 08:26:02 [scrapy_fake_useragent.middleware] INFO: Error loading User-Agent provider: scrapy_fake_useragent.providers.FakeUserAgentProvider\n",
      "2024-04-08 08:26:02 [scrapy_fake_useragent.middleware] INFO: Unable to load any of the User-Agent providers\n",
      "2024-04-08 08:26:02 [scrapy_fake_useragent.middleware] INFO: Using '<class 'scrapy_fake_useragent.providers.FixedUserAgentProvider'>' as the User-Agent provider\n",
      "2024-04-08 08:26:02 [scrapy_fake_useragent.middleware] INFO: Error loading User-Agent provider: scrapy_fake_useragent.providers.FakeUserAgentProvider\n",
      "2024-04-08 08:26:02 [scrapy_fake_useragent.middleware] INFO: Unable to load any of the User-Agent providers\n",
      "2024-04-08 08:26:02 [scrapy_fake_useragent.middleware] INFO: Using '<class 'scrapy_fake_useragent.providers.FixedUserAgentProvider'>' as the User-Agent provider\n",
      "2024-04-08 08:26:02 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware',\n",
      " 'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-04-08 08:26:02 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-04-08 08:26:02 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-04-08 08:26:02 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-04-08 08:26:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-04-08 08:26:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2024-04-08 08:26:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.vietnamworks.com/nhan-vien-trien-khai-giai-phap-giam-sat-he-thong-cntt-1762185-jv?source=searchResults&searchType=2&placement=1762185&sortBy=latest> (referer: None)\n",
      "2024-04-08 08:26:03 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.vietnamworks.com/nhan-vien-trien-khai-giai-phap-giam-sat-he-thong-cntt-1762185-jv?source=searchResults&searchType=2&placement=1762185&sortBy=latest>\n",
      "\n",
      "{'url': 'https://www.vietnamworks.com/nhan-vien-trien-khai-giai-phap-giam-sat-he-thong-cntt-1762185-jv?source=searchResults&searchType=2&placement=1762185&sortBy=latest', 'job_name': 'Nhân Viên Triển Khai Giải Pháp Giám Sát Hệ Thống CNTT', 'company_name': 'Sota Việt Nam', 'salary': '$500 - $1000', 'end_date': '2024-05-07', 'address': 'Tầng 2, cánh 3 Trung Yên Plaza, Số 1 Trung Hòa, Cầu Giấy, Hà Nội', 'NGÀY ĐĂNG': ['07/04/2024'], 'NGÀNH NGHỀ': ['Công Nghệ Thông Tin/Viễn Thông', ' > ', 'Phần Mềm Máy Tính'], 'LĨNH VỰC': ['Phần Mềm CNTT/Dịch vụ Phần mềm'], 'CẤP BẬC': ['Nhân viên'], 'KỸ NĂNG': ['Hệ Thống Quản Lý Giám Sát, Điều Khiển Tự Động, IT Phần Mềm, Công Nghệ Thông Tin, IT Security'], 'NGÔN NGỮ TRÌNH BÀY HỒ SƠ': ['Bất kỳ'], 'Mô tả công việc': [['p-\\tTìm hiểu khảo sát, thu thập thông tin hệ thống CNTT và các nhu cầu giám sát của khách hàng.', '-\\tTư vấn, thiết kế hệ thống giám sát cho các hệ thống CNTT.', '-\\tTriển khai cài đặt, cấu hình, lập trình cho các giải pháp giám sát hệ thống CNTT.', '-\\tXây dựng các tài liệu kỹ thuật liên quan; Thực hiện đào tạo và chuyển giao hệ thống; hỗ trợ công tác vận hành hệ thống...', '-\\tTham gia các dự án CNTT khác của công ty theo phân công./p']], 'Yêu cầu công việc': [['p-\\tTốt nghiệp Đại học chuyên ngành CNTT', '-\\tSử dụng thành thạo hệ điều hành Windows Server, Linux, Shell scripts', '-\\tCó kinh nghiệm triển khai, vận hành các giải pháp monitor: Solarwinds, ManageEngine, Splunk và sử dụng được một trong các NNLT Python, Java là các lợi thế./p']]}\n",
      "2024-04-08 08:26:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.vietnamworks.com/software-engineer--1762790-jv?source=searchResults&searchType=2&placement=1762790&sortBy=latest> (referer: https://www.vietnamworks.com/nhan-vien-trien-khai-giai-phap-giam-sat-he-thong-cntt-1762185-jv?source=searchResults&searchType=2&placement=1762185&sortBy=latest)\n",
      "2024-04-08 08:26:03 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.vietnamworks.com/software-engineer--1762790-jv?source=searchResults&searchType=2&placement=1762790&sortBy=latest>\n",
      "\n",
      "{'url': 'https://www.vietnamworks.com/software-engineer--1762790-jv?source=searchResults&searchType=2&placement=1762790&sortBy=latest', 'job_name': 'Software Engineer', 'company_name': 'Ezom App', 'salary': 'Negotiable', 'end_date': '2024-05-06', 'address': None, 'POSTED DATE': ['06/04/2024'], 'JOB FUNCTION': ['Information Technology/Telecommunications', ' > ', 'Software Developer'], 'JOB INDUSTRY': ['IT Software/SaaS'], 'JOB LEVEL': ['Experienced (non-manager)'], 'SKILL': ['JavaScript, SQL, Typescript, React, React Native'], 'PREFERRED LANGUAGE': ['English'], 'Job description': [[\"We seek a talented and motivated Software Engineer to join our dynamic team at EzOM. As a trusted provider of inventory management and order fulfilment solutions to Australia's leading e-commerce businesses, we offer an exciting opportunity to work with cutting-edge technologies.\", 'Join our team of world-class engineers and machine-learning research experts who have contributed to projects with millions and billions of users, including widely recognised platforms like Google Chrome and have made ground-breaking research and publications in leading conferences, including ICML, ICLR and AAAI.', 'Responsibilities:', '- Develop and maintain high-quality software applications using React, GraphQL, TypeScript, SQL, Node.js, and other relevant technologies.', '- Mentor and coach junior software engineers.', '- Participate in code reviews, diligently identify and address bugs, and propose innovative solutions to ensure top-notch software development practices.', '- Engage in the entire software development lifecycle, from gathering requirements and designing solutions to implementing, testing, and deploying software systems.', '- Stay at the forefront of industry trends and emerging technologies, continuously expanding your technical skills and knowledge.']], 'Job requirements': [['- 3+ years of software development experience.', '- Strong problem-solving abilities, with the capacity to think critically and resolve complex technical challenges.', '- Excellent collaboration and communication skills, enabling effective teamwork and seamless stakeholder interaction.', '- Self-motivated and proactive approach to learning, coupled with a keenness to remain up-to-date with emerging technologies.', '- Demonstrated experience and proficiency in React, GraphQL, TypeScript, SQL and Node.js.', '- Exhibited senior expertise in software architecture, design and implementation, delivering resilient and sustainable code to solve business problems', '- Displayed leadership skills, including mentoring and guiding junior engineers.', \"- Achieved success with leading complex software projects' design, development and delivery.\", \"- (Preferred but not mandatory) Bachelor's/ Master’s degree in Computer Science, Software Engineering, or a related field.\"]]}\n",
      "2024-04-08 08:26:03 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2024-04-08 08:26:03 [scrapy.extensions.feedexport] INFO: Stored json feed (2 items) in: json/out.json\n",
      "2024-04-08 08:26:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2050,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 291724,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 0.833811,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2024, 4, 8, 1, 26, 3, 525548, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 556397,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 2,\n",
      " 'log_count/DEBUG': 8,\n",
      " 'log_count/INFO': 17,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2024, 4, 8, 1, 26, 2, 691737, tzinfo=datetime.timezone.utc)}\n",
      "2024-04-08 08:26:03 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!scrapy crawl vnw_i4_crawler -o json/out.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "!python json/format_out_data.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSAIenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
