{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "JSONLoader.__init__() missing 1 required positional argument: 'jq_schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 187\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# embedding_model = load_embedding_model()\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# retriever = create_parent_retriever(docs, embedding_model)\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# reranker_model = load_reranker_model()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# )[0]\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# print(\"context:\\n\", context, \"\\n\", \"=\" * 50, \"\\n\")\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# from jsonargparse import CLI\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# CLI(main)\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 170\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(file, query, llm_name)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(\n\u001b[1;32m    165\u001b[0m     file: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/crawl/train_jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    166\u001b[0m     query: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    167\u001b[0m     llm_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    168\u001b[0m ):\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# docs = load_pdf(files=file)\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[43mload_jsonl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mprint\u001b[39m(docs)\n",
      "Cell \u001b[0;32mIn[16], line 72\u001b[0m, in \u001b[0;36mload_jsonl\u001b[0;34m(files)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_jsonl\u001b[39m(\n\u001b[1;32m     69\u001b[0m     files: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/crawl/train_test.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(files, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 72\u001b[0m         loader \u001b[38;5;241m=\u001b[39m \u001b[43mJSONLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m loader\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     77\u001b[0m     loaders \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     78\u001b[0m         JSONLoader(\n\u001b[1;32m     79\u001b[0m             file,\n\u001b[1;32m     80\u001b[0m         )\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files\n\u001b[1;32m     82\u001b[0m     ]\n",
      "\u001b[0;31mTypeError\u001b[0m: JSONLoader.__init__() missing 1 required positional argument: 'jq_schema'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/space/hotel/phit/personal/experiments/weights\"\n",
    "os.environ[\"TORCH_HOME\"] = \"/space/hotel/phit/personal/experiments/weights\"\n",
    "\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "from langchain.callbacks import FileCallbackHandler\n",
    "from langchain.retrievers import ContextualCompressionRetriever, ParentDocumentRetriever\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader, JSONLoader\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "from langchain_core.documents import Document\n",
    "from loguru import logger\n",
    "from rich import print\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "from unstructured.cleaners.core import clean_extra_whitespace, group_broken_paragraphs\n",
    "\n",
    "logfile = \"log/output.log\"\n",
    "logger.add(logfile, colorize=True, enqueue=True)\n",
    "handler = FileCallbackHandler(logfile)\n",
    "\n",
    "\n",
    "persist_directory = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RAGException(Exception):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "\n",
    "def rerank_docs(reranker_model, query, retrieved_docs):\n",
    "    query_and_docs = [(query, r.page_content) for r in retrieved_docs]\n",
    "    scores = reranker_model.predict(query_and_docs)\n",
    "    return sorted(list(zip(retrieved_docs, scores)), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "def load_pdf(\n",
    "    files: Union[str, List[str]] = \"../data/cv/Bui Tien Phat resume (1).pdf\"\n",
    ") -> List[Document]:\n",
    "    if isinstance(files, str):\n",
    "        loader = UnstructuredFileLoader(\n",
    "            files,\n",
    "            post_processors=[clean_extra_whitespace, group_broken_paragraphs],\n",
    "        )\n",
    "        return loader.load()\n",
    "\n",
    "    loaders = [\n",
    "        UnstructuredFileLoader(\n",
    "            file,\n",
    "            post_processors=[clean_extra_whitespace, group_broken_paragraphs],\n",
    "        )\n",
    "        for file in files\n",
    "    ]\n",
    "    docs = []\n",
    "    for loader in loaders:\n",
    "        docs.extend(\n",
    "            loader.load(),\n",
    "        )\n",
    "    return docs\n",
    "\n",
    "# Define the metadata extraction function.\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "\n",
    "    metadata[\"urls\"] = record.get(\"urls\")\n",
    "    metadata[\"job_name\"] = record.get(\"job_name\")\n",
    "    metadata[\"company_name\"] = record.get(\"company_name\")\n",
    "    metadata[\"address\"] = record.get(\"address\")\n",
    "    metadata[\"salary\"] = record.get(\"salary\")\n",
    "    metadata[\"remaining\"] = record.get(\"remaining\")\n",
    "    metadata[\"Mô tả công việc\"] = record.get(\"Mô tả công việc\")\n",
    "    metadata[\"Yêu cầu ứng viên\"] = record.get(\"Yêu cầu ứng viên\")\n",
    "    metadata[\"Quyền lợi\"] = record.get(\"Quyền lợi\")\n",
    "    metadata[\"Địa điểm làm việc\"] = record.get(\"Địa điểm làm việc\")\n",
    "    metadata[\"Cách thức ứng tuyển\"] = record.get(\"Cách thức ứng tuyển\")\n",
    "    metadata[\"Cấp bậc\"] = record.get(\"Cấp bậc\")\n",
    "    metadata[\"Kinh nghiệm\"] = record.get(\"Kinh nghiệm\")\n",
    "    metadata[\"Số lượng tuyển\"] = record.get(\"Số lượng tuyển\")\n",
    "    metadata[\"Hình thức làm việc\"] = record.get(\"Hình thức làm việc\")\n",
    "    metadata[\"Giới tính\"] = record.get(\"Giới tính\")\n",
    "    \n",
    "    \n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def load_jsonl(\n",
    "    files: Union[str, List[str]] = \"../data/crawl/train_test.jsonl\"\n",
    ") -> List[Document]:\n",
    "    if isinstance(files, str):\n",
    "        loader = JSONLoader(\n",
    "            files,\n",
    "            json_lines=True,\n",
    "            jq_schema='.messages[]',\n",
    "            content_key=\"content\",\n",
    "            metadata_func=metadata_func\n",
    "        )\n",
    "        return loader.load()\n",
    "\n",
    "    loaders = [\n",
    "        JSONLoader(\n",
    "            file,\n",
    "            json_lines=True,\n",
    "            jq_schema='.messages[]',\n",
    "            content_key=\"content\",\n",
    "            metadata_func=metadata_func\n",
    "        )\n",
    "        for file in files\n",
    "    ]\n",
    "    docs = []\n",
    "    for loader in loaders:\n",
    "        docs.extend(\n",
    "            loader.load(),\n",
    "        )\n",
    "    return docs\n",
    "\n",
    "def create_parent_retriever(\n",
    "    docs: List[Document], embeddings_model: HuggingFaceBgeEmbeddings()\n",
    "):\n",
    "    parent_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\\n\", \"\\n\\n\"],\n",
    "        chunk_size=2000,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    # This text splitter is used to create the child documents\n",
    "    child_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\\n\", \"\\n\\n\"],\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=300,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    # The vectorstore to use to index the child chunks\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=\"split_documents\",\n",
    "        embedding_function=embeddings_model,\n",
    "        persist_directory=persist_directory,\n",
    "    )\n",
    "    # The storage layer for the parent documents\n",
    "    store = InMemoryStore()\n",
    "    retriever = ParentDocumentRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        child_splitter=child_splitter,\n",
    "        parent_splitter=parent_splitter,\n",
    "        k=10,\n",
    "    )\n",
    "    retriever.add_documents(docs)\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def retrieve_context(query, retriever, reranker_model):\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "    if len(retrieved_docs) == 0:\n",
    "        raise RAGException(\n",
    "            f\"Couldn't retrieve any relevant document with the query `{query}`. Try modifying your question!\"\n",
    "        )\n",
    "    reranked_docs = rerank_docs(\n",
    "        query=query, retrieved_docs=retrieved_docs, reranker_model=reranker_model\n",
    "    )\n",
    "    return reranked_docs\n",
    "\n",
    "\n",
    "def load_embedding_model(\n",
    "    model_name: str = \"BAAI/bge-large-en-v1.5\", device: str = \"cuda\"\n",
    ") -> HuggingFaceBgeEmbeddings:\n",
    "    model_kwargs = {\"device\": device}\n",
    "    encode_kwargs = {\n",
    "        \"normalize_embeddings\": True\n",
    "    }  # set True to compute cosine similarity\n",
    "    embedding_model = HuggingFaceBgeEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "    )\n",
    "    return embedding_model\n",
    "\n",
    "\n",
    "def load_reranker_model(\n",
    "    reranker_model_name: str = \"BAAI/bge-reranker-large\", device: str = \"cuda\"\n",
    ") -> CrossEncoder:\n",
    "    reranker_model = CrossEncoder(\n",
    "        model_name=reranker_model_name, max_length=512, device=device\n",
    "    )\n",
    "    return reranker_model\n",
    "\n",
    "\n",
    "def main(\n",
    "    file: str = \"../data/crawl/train_jsonl\",\n",
    "    query: Optional[str] = None,\n",
    "    llm_name=\"mistral\",\n",
    "):\n",
    "    # docs = load_pdf(files=file)\n",
    "    docs = load_jsonl(file)\n",
    "    print(docs)\n",
    "\n",
    "    # embedding_model = load_embedding_model()\n",
    "    # retriever = create_parent_retriever(docs, embedding_model)\n",
    "    # reranker_model = load_reranker_model()\n",
    "\n",
    "    # context = retrieve_context(\n",
    "    #     query, retriever=retriever, reranker_model=reranker_model\n",
    "    # )[0]\n",
    "    # print(\"context:\\n\", context, \"\\n\", \"=\" * 50, \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # from jsonargparse import CLI\n",
    "\n",
    "    # CLI(main)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Data Science/AI Engineer\\n\\nTIEN PHAT BUI\\n\\n0962980173 | buitienphat2462002@gmail.com https://www.linkedin.com/in/tien-phat-bui-ba3421222/ https://github.com/ShynBui/ Go Vap district, Ho Chi Minh city\\n\\nEducation\\n\\nB ACH EL OR OF IN FOR M ATION TECH N OL O G Y(Expected 05/2024) – Ho Chi Minh City Open University – 97 Vo Van Tan, Ward 6, District 3, Ho Chi Minh City GPA: 3.76/4.00 (Highest in Major) Majors: Information Technology Relevant Coursework: Machine Learning, Data Visualization and Communication, Database Systems and SQL, Natural Language Processing, Computer Vision, Deep Learning and Neural Networks\\n\\nSkills\\n\\nPython Libraries and frameworks: pandas, matplotlib, numpy, BeutifulSoup4, re, os, scikit-learn, Hugging Face, transformers, PyTorch, Python-Flask, LangChain, NLTK Big Data Apache Hadoop, PySpark SQL Software: SQLite, MySQL, SQL, PostgreSQL Power Bi Tools: Power Query, Visualize C++ Libraries: STL, Boot C++, Armadillo\\n\\nProjects\\n\\nLEGAL SEAR CH AN D R ES PON SE PLATFOR M – Group competition project\\n\\nNovember 2023 \\n\\n-\\n\\n now\\n\\n● Provides users with quick access to legal information, delivers reliable answers to legal queries, and enables efficient navigation through legal codes and documents\\n\\n● Utilizes ReactJS for front\\n\\n-end development, employs Python Flask for back\\n\\n-end functionality, and enhances its capabilities with complementary integration tools such as CI/CD (Continuous Integration/Continuous Deployment) and Docker for streamlined deployment processes, ensuring efficient and reliable operation\\n\\n● Project management, ensuring a waterfall system development process, data search and preprocessing, training deep learning models based on PhoBERT and LangChain for question\\n\\n-answering tasks for answering legal and training another model to classify \"legal definition\" sentences in legal documents, taking on the role of data designer using MySQL, and visualize the model training process for reporting using matplotlib\\n\\nWinning third prize in the Vietnam Student Informatics Olympiad - Open source competition, the project attracted the jury for its creativity and was considered to have the most potential. The product is open source code that is being invested in, developed and raised capital The article introduces the project: https://vfossa.vn/tin-tuc/code-heroes-nen-tang-tra-cuu-va-giai-dap-phap-luat-ung-dung-sang- tao-cac-mo-hinh-ngon-ngu-lon-698.html\\n\\nAN AUTOM ATICALLY AN S WER IN G EN GLISH AN D VIETN AM ESE R EA D IN G COM PR EH EN SION S YSTEM – Personal June 2021 - January 2022 Project\\n\\n● Assisting users in automatically answering Vietnamese or English reading comprehension questions by providing the system with\\n\\na passage of information (provided by the user), from which the system extracts information to answer the user\\'s queries ● Using Hugging Face as a model repository, we employ a BERT-based model for answering English reading comprehension\\n\\nUsing Hugging Face as a model repository, we employ a BERT-based model for answering English reading comprehension questions and a PhoBERT-based model for answering Vietnamese reading comprehension questions. Gradio is utilized to design an interactive interface for user interaction\\n\\nquestions and a PhoBERT-based model for answering Vietnamese reading comprehension questions. Gradio is utilized to design an interactive interface for user interaction Taking on the role of data preparation, I utilize available datasets and scrape data from the web using BeautifulSoup4. Data processing is performed using Python, while model fine-tuning is carried out through the use of Transformers and PyTorch. Model evaluation and visualization of evaluation results are conducted using matplotlib The English question-answering model achieved a significant 78% accuracy when applied to multiple-choice questions. The Vietnamese question-answering model also achieved an exact match accuracy of 72%\\n\\n\\n\\n\\n\\nOTH ER PR OJECT\\n\\nOnline library ● ● Website for book sales management ● Vietnamese toxic comment classification\\n\\nAchievements and Awards\\n\\nTH E 2022 ICPC ASI A H O CH I M INH CITY – HCMUTE – Ho Chi Minh City\\n\\nDecember 2022\\n\\n● Ranked among the top 260 / 470 teams in the ICPC Asia Regionals, showcasing problem\\n\\n-solving skills and algorithmic proficiency.\\n\\nTH E 2023 ICPC ASI A H UE CITY – Hue Uni ver si ty Of Sci ence – Hue City\\n\\nDecember 2023\\n\\nThird prize in the Vietnam Student Informatics Olympiad - Open source competition. Solution building project \"Developing an application to support searching and asking questions about legal knowledge based on the Legal Code and Legal Document Database\", applying and refining LLMs models to training chat-bot to answer legal questions\\n\\n\\n\\nTH E 24H PR OGR AMM IN G – Ho Chi Mi nh Ci ty Open Uni ver si ty – Ho Chi Mi nh Ci ty\\n\\nApril 2023\\n\\n● Champion of the 24h Programming Contest. Build an application that applies algorithms to suggest user habits using machine learning and visualize daily activities and consumption habits using matplotlib, helping users manage time and money\\n\\nWork Experience\\n\\nDATA AN AL YST – I SD CORP – Tan Binh, HCM City\\n\\nJune 2023 \\n\\n-\\n\\n August 2023\\n\\n● Created interactive dashboards and reports using Power BI, matplotlib to visualize and communicate data\\n\\n-driven insights to stakeholders\\n\\n● Conducted data cleaning, preprocessing, and feature engineering to ensure data quality and accuracy. Utilized statistical analysis techniques and machine learning algorithms to develop predictive models for business forecasting\\n\\n● Develop charts in the CRM system to evaluate the business performance of the enterprise. Proficient in using SQL techniques to query data, proficiently applying matplotlib, Power BI and scikit\\n\\n-learn machine learning techniques to enhance customers\\' decision making process\\n\\nOptimized data processing workflows, resulting in a 10% reduction in data processing time ● Developed a comprehensive customer segmentation model that improved targeted marketing campaigns and increased\\n\\ncustomer engagement\\n\\nCertification\\n\\nDATA SCIEN C E AN D M ACH IN E L EAR N IN G CER T IFICATES\\n\\nHo Chi Minh City University of Natural Sciences Informatics Center ● Certification Date: 2023 ● Description: Helps master Python programming, use various tools and libraries for data science and machine learning engineer, apply data preprocessing techniques, implement machine learning algorithms to solve real problems economics, and work with Big Data technologies such as Spark and PySpark\\n\\nICPC PAR TIC IPATI ON C ER TIFICAT E\\n\\n● Certification Date: October 2022 \\n\\n● Description: The ICPC (International Collegiate Programming Contest) certificate is awarded to participants who have competed in the ICPC regional or world finals competitions. It serves as a recognition of their accomplishment and participation in this prestigious programming contest', metadata={'source': '../data/cv/Bui Tien Phat resume (1).pdf'})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_pdf(\n",
    "    files: Union[str, List[str]] = \"../data/cv/Bui Tien Phat resume (1).pdf\"\n",
    ") -> List[Document]:\n",
    "    if isinstance(files, str):\n",
    "        loader = UnstructuredFileLoader(\n",
    "            files,\n",
    "            post_processors=[clean_extra_whitespace, group_broken_paragraphs],\n",
    "        )\n",
    "        return loader.load()\n",
    "\n",
    "    loaders = [\n",
    "        UnstructuredFileLoader(\n",
    "            file,\n",
    "            post_processors=[clean_extra_whitespace, group_broken_paragraphs],\n",
    "        )\n",
    "        for file in files\n",
    "    ]\n",
    "    docs = []\n",
    "    for loader in loaders:\n",
    "        docs.extend(\n",
    "            loader.load(),\n",
    "        )\n",
    "    return docs\n",
    "\n",
    "load_pdf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
