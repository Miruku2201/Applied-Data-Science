{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected the jq schema to result in a list of objects (dict)                     with the key `job_description`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 240\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, context, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# from jsonargparse import CLI\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# CLI(main)\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# main(query=\"What is the job description for Network Engineer?\")\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is job description in Da Nang ?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 222\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(file, query, llm_name)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(\n\u001b[1;32m    217\u001b[0m     file: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/crawl/train_test.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    218\u001b[0m     query: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m     llm_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m ):\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# docs = load_pdf(files=file)\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[43mload_jsonl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/crawl/train_test.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# print(docs)\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     embedding_model \u001b[38;5;241m=\u001b[39m load_embedding_model()\n",
      "Cell \u001b[0;32mIn[30], line 120\u001b[0m, in \u001b[0;36mload_jsonl\u001b[0;34m(files)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(files, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    112\u001b[0m     loader \u001b[38;5;241m=\u001b[39m JSONLoader(\n\u001b[1;32m    113\u001b[0m         files,\n\u001b[1;32m    114\u001b[0m         json_lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m         metadata_func\u001b[38;5;241m=\u001b[39mmetadata_func\n\u001b[1;32m    119\u001b[0m     )\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m loaders \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    123\u001b[0m     JSONLoader(\n\u001b[1;32m    124\u001b[0m         file,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files\n\u001b[1;32m    131\u001b[0m ]\n\u001b[1;32m    132\u001b[0m docs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/space/hotel/phit/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/document_loaders/base.py:29\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/space/hotel/phit/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_community/document_loaders/json_loader.py:78\u001b[0m, in \u001b[0;36mJSONLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m line:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse(line, index):\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m doc\n\u001b[1;32m     80\u001b[0m         index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/space/hotel/phit/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_community/document_loaders/json_loader.py:94\u001b[0m, in \u001b[0;36mJSONLoader._parse\u001b[0;34m(self, content, index)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Perform some validation\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# This is not a perfect validation, but it should catch most cases\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# and prevent the user from getting a cryptic error later on.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_content_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_metadata_func(data)\n",
      "File \u001b[0;32m/space/hotel/phit/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_community/document_loaders/json_loader.py:159\u001b[0m, in \u001b[0;36mJSONLoader._validate_content_key\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected the jq schema to result in a list of objects (dict), \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124m            so sample must be a dict but got `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(sample)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m     )\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_content_key_jq_parsable\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sample\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_key) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    158\u001b[0m ):\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected the jq schema to result in a list of objects (dict) \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124m            with the key `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_content_key_jq_parsable\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjq\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_key)\u001b[38;5;241m.\u001b[39minput(sample)\u001b[38;5;241m.\u001b[39mtext() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    166\u001b[0m ):\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected the jq schema to result in a list of objects (dict) \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124m            with the key `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` which should be parsable by jq\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected the jq schema to result in a list of objects (dict)                     with the key `job_description`"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/space/hotel/phit/personal/experiments/weights\"\n",
    "os.environ[\"TORCH_HOME\"] = \"/space/hotel/phit/personal/experiments/weights\"\n",
    "\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "from langchain.callbacks import FileCallbackHandler\n",
    "from langchain.retrievers import ContextualCompressionRetriever, ParentDocumentRetriever\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader, JSONLoader\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS , Chroma\n",
    "# from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from loguru import logger\n",
    "from rich import print\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "from unstructured.cleaners.core import clean_extra_whitespace, group_broken_paragraphs\n",
    "\n",
    "logfile = \"log/output.log\"\n",
    "logger.add(logfile, colorize=True, enqueue=True)\n",
    "handler = FileCallbackHandler(logfile)\n",
    "\n",
    "\n",
    "persist_directory = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RAGException(Exception):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "\n",
    "def rerank_docs(reranker_model, query, retrieved_docs):\n",
    "    query_and_docs = [(query, r.page_content) for r in retrieved_docs]\n",
    "    scores = reranker_model.predict(query_and_docs)\n",
    "    return sorted(list(zip(retrieved_docs, scores)), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "def load_pdf(\n",
    "    files: Union[str, List[str]] = \"../data/cv/Bui Tien Phat resume (1).pdf\"\n",
    ") -> List[Document]:\n",
    "    if isinstance(files, str):\n",
    "        loader = UnstructuredFileLoader(\n",
    "            files,\n",
    "            post_processors=[clean_extra_whitespace, group_broken_paragraphs],\n",
    "        )\n",
    "        return loader.load()\n",
    "\n",
    "    loaders = [\n",
    "        UnstructuredFileLoader(\n",
    "            file,\n",
    "            post_processors=[clean_extra_whitespace, group_broken_paragraphs],\n",
    "        )\n",
    "        for file in files\n",
    "    ]\n",
    "    docs = []\n",
    "    for loader in loaders:\n",
    "        docs.extend(\n",
    "            loader.load(),\n",
    "        )\n",
    "    return docs\n",
    "\n",
    "VIETNAMWORKS = ['_id', 'url', 'job_name', 'company_name', 'salary', 'end_date',\n",
    "       'address', 'posted_date', 'job_function', 'job_industry', 'job_level',\n",
    "       'skill', 'preferred_language', 'job_requirements'] # job_description\n",
    "\n",
    "TOPCV = ['_id', 'urls', 'job_name', 'company_name', 'address', 'salary',\n",
    "       'remaining', 'job_requirements', 'benefits',\n",
    "       'application_method', 'level', 'experience', 'number_of_recruitment',\n",
    "       'work_form', 'gender', 'working_time'] # job_description\n",
    "\n",
    "# Define the metadata extraction function.\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    \n",
    "    # metadata[\"urls\"] = record.get(\"urls\")\n",
    "    # metadata[\"job_name\"] = record.get(\"job_name\")\n",
    "    # metadata[\"company_name\"] = record.get(\"company_name\")\n",
    "    # metadata[\"address\"] = record.get(\"address\")\n",
    "    # metadata[\"salary\"] = record.get(\"salary\")\n",
    "    # metadata[\"remaining\"] = record.get(\"remaining\")\n",
    "    # # metadata[\"\"] = record.get(\"Mô tả công việc\")\n",
    "    # metadata[\"Yêu cầu ứng viên\"] = record.get(\"Yêu cầu ứng viên\")\n",
    "    # metadata[\"Quyền lợi\"] = record.get(\"Quyền lợi\")\n",
    "    # metadata[\"Địa điểm làm việc\"] = record.get(\"Địa điểm làm việc\")\n",
    "    # metadata[\"Cách thức ứng tuyển\"] = record.get(\"Cách thức ứng tuyển\")\n",
    "    # metadata[\"Cấp bậc\"] = record.get(\"Cấp bậc\")\n",
    "    # metadata[\"Kinh nghiệm\"] = record.get(\"Kinh nghiệm\")\n",
    "    # metadata[\"Số lượng tuyển\"] = record.get(\"Số lượng tuyển\")\n",
    "    # metadata[\"Hình thức làm việc\"] = record.get(\"Hình thức làm việc\")\n",
    "    # metadata[\"Giới tính\"] = record.get(\"Giới tính\")\n",
    "    \n",
    "    for key in VIETNAMWORKS:\n",
    "        metadata[key] = record.get(key)\n",
    "    \n",
    "    metadata = {key: f'{\" \".join(val[0]) if isinstance(val, list) else val}' for key, val in metadata.items()}\n",
    "\n",
    "    \n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def load_jsonl(\n",
    "    files: Union[str, List[str]] = \"../data/crawl/train_test.jsonl\"\n",
    ") -> List[Document]:\n",
    "    if isinstance(files, str):\n",
    "        loader = JSONLoader(\n",
    "            files,\n",
    "            json_lines=True,\n",
    "            jq_schema='.',\n",
    "            content_key=\"job_description\", \n",
    "            text_content=False,\n",
    "            metadata_func=metadata_func\n",
    "        )\n",
    "        return loader.load()\n",
    "\n",
    "    loaders = [\n",
    "        JSONLoader(\n",
    "            file,\n",
    "            json_lines=True,\n",
    "            jq_schema='.messages[]',\n",
    "            content_key=\"content\",\n",
    "            metadata_func=metadata_func\n",
    "        )\n",
    "        for file in files\n",
    "    ]\n",
    "    docs = []\n",
    "    for loader in loaders:\n",
    "        docs.extend(\n",
    "            loader.load(),\n",
    "        )\n",
    "    return docs\n",
    "\n",
    "def create_parent_retriever(\n",
    "    docs: List[Document], embeddings_model: HuggingFaceBgeEmbeddings()\n",
    "):\n",
    "    parent_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\\n\", \"\\n\\n\"],\n",
    "        chunk_size=2000,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    # This text splitter is used to create the child documents\n",
    "    child_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\\n\", \"\\n\\n\"],\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=300,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    # The vectorstore to use to index the child chunks\n",
    "    # vectorstore = Chroma(\n",
    "    #     collection_name=\"split_documents\",\n",
    "    #     embedding_function=embeddings_model,\n",
    "    #     persist_directory=persist_directory,\n",
    "    # ).as_retriever()\n",
    "    \n",
    "    vectorstore = Chroma.from_documents(docs, embeddings_model)#.as_retriever()\n",
    "    print(\"vectorstore: \", vectorstore)\n",
    "    # The storage layer for the parent documents\n",
    "    store = InMemoryStore()\n",
    "    retriever = ParentDocumentRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        child_splitter=child_splitter,\n",
    "        parent_splitter=parent_splitter,\n",
    "        k=10,\n",
    "    )\n",
    "    retriever.add_documents(docs)\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def retrieve_context(query, retriever, reranker_model):\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "    if len(retrieved_docs) == 0:\n",
    "        raise RAGException(\n",
    "            f\"Couldn't retrieve any relevant document with the query `{query}`. Try modifying your question!\"\n",
    "        )\n",
    "    reranked_docs = rerank_docs(\n",
    "        query=query, retrieved_docs=retrieved_docs, reranker_model=reranker_model\n",
    "    )\n",
    "    return reranked_docs\n",
    "\n",
    "\n",
    "def load_embedding_model(\n",
    "    model_name: str = \"BAAI/bge-large-en-v1.5\", device: str = \"cuda\"\n",
    ") -> HuggingFaceBgeEmbeddings:\n",
    "    model_kwargs = {\"device\": device}\n",
    "    encode_kwargs = {\n",
    "        \"normalize_embeddings\": True\n",
    "    }  # set True to compute cosine similarity\n",
    "    embedding_model = HuggingFaceBgeEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "    )\n",
    "    return embedding_model\n",
    "\n",
    "\n",
    "def load_reranker_model(\n",
    "    reranker_model_name: str = \"BAAI/bge-reranker-large\", device: str = \"cuda\"\n",
    ") -> CrossEncoder:\n",
    "    reranker_model = CrossEncoder(\n",
    "        model_name=reranker_model_name, max_length=512, device=device\n",
    "    )\n",
    "    return reranker_model\n",
    "\n",
    "\n",
    "def main(\n",
    "    file: str = \"../data/crawl/train_test.jsonl\",\n",
    "    query: Optional[str] = None,\n",
    "    llm_name=\"mistral\",\n",
    "):\n",
    "    # docs = load_pdf(files=file)\n",
    "    docs = load_jsonl(\"../data/crawl/vnw.jsonl\")\n",
    "    # print(docs)\n",
    "\n",
    "    embedding_model = load_embedding_model()\n",
    "    retriever = create_parent_retriever(docs, embedding_model)\n",
    "    reranker_model = load_reranker_model()\n",
    "\n",
    "    context = retrieve_context(\n",
    "        query, retriever=retriever, reranker_model=reranker_model\n",
    "    )[0]\n",
    "    print(\"context:\\n\", context, \"\\n\", \"=\" * 50, \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # from jsonargparse import CLI\n",
    "\n",
    "    # CLI(main)\n",
    "    # main(query=\"What is the job description for Network Engineer?\")\n",
    "    main(query=\"What is job description in Da Nang ?\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_jsonl(\"../data/crawl/vnw.jsonl\")\n",
    "print(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
