{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ParentDocumentRetriever\nvectorstore\n  instance of VectorStore expected (type=type_error.arbitrary_type; expected_arbitrary_type=VectorStore)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 226\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, context, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# from jsonargparse import CLI\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# CLI(main)\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the job description for Network Engineer?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 213\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(file, query, llm_name)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# print(docs)\u001b[39;00m\n\u001b[1;32m    212\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m load_embedding_model()\n\u001b[0;32m--> 213\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_parent_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m reranker_model \u001b[38;5;241m=\u001b[39m load_reranker_model()\n\u001b[1;32m    216\u001b[0m context \u001b[38;5;241m=\u001b[39m retrieve_context(\n\u001b[1;32m    217\u001b[0m     query, retriever\u001b[38;5;241m=\u001b[39mretriever, reranker_model\u001b[38;5;241m=\u001b[39mreranker_model\n\u001b[1;32m    218\u001b[0m )[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[2], line 155\u001b[0m, in \u001b[0;36mcreate_parent_retriever\u001b[0;34m(docs, embeddings_model)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# The storage layer for the parent documents\u001b[39;00m\n\u001b[1;32m    154\u001b[0m store \u001b[38;5;241m=\u001b[39m InMemoryStore()\n\u001b[0;32m--> 155\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[43mParentDocumentRetriever\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocstore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchild_splitter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchild_splitter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_splitter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_splitter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m retriever\u001b[38;5;241m.\u001b[39madd_documents(docs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retriever\n",
      "File \u001b[0;32m/space/hotel/phit/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m/space/hotel/phit/miniconda3/envs/llm/lib/python3.10/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ParentDocumentRetriever\nvectorstore\n  instance of VectorStore expected (type=type_error.arbitrary_type; expected_arbitrary_type=VectorStore)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/space/hotel/phit/personal/experiments/weights\"\n",
    "os.environ[\"TORCH_HOME\"] = \"/space/hotel/phit/personal/experiments/weights\"\n",
    "\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "from langchain.callbacks import FileCallbackHandler\n",
    "from langchain.retrievers import ContextualCompressionRetriever, ParentDocumentRetriever\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader, JSONLoader\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS #, Chroma\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from loguru import logger\n",
    "from rich import print\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "from unstructured.cleaners.core import clean_extra_whitespace, group_broken_paragraphs\n",
    "\n",
    "logfile = \"log/output.log\"\n",
    "logger.add(logfile, colorize=True, enqueue=True)\n",
    "handler = FileCallbackHandler(logfile)\n",
    "\n",
    "\n",
    "persist_directory = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RAGException(Exception):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "\n",
    "def rerank_docs(reranker_model, query, retrieved_docs):\n",
    "    query_and_docs = [(query, r.page_content) for r in retrieved_docs]\n",
    "    scores = reranker_model.predict(query_and_docs)\n",
    "    return sorted(list(zip(retrieved_docs, scores)), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "def load_pdf(\n",
    "    files: Union[str, List[str]] = \"../data/cv/Bui Tien Phat resume (1).pdf\"\n",
    ") -> List[Document]:\n",
    "    if isinstance(files, str):\n",
    "        loader = UnstructuredFileLoader(\n",
    "            files,\n",
    "            post_processors=[clean_extra_whitespace, group_broken_paragraphs],\n",
    "        )\n",
    "        return loader.load()\n",
    "\n",
    "    loaders = [\n",
    "        UnstructuredFileLoader(\n",
    "            file,\n",
    "            post_processors=[clean_extra_whitespace, group_broken_paragraphs],\n",
    "        )\n",
    "        for file in files\n",
    "    ]\n",
    "    docs = []\n",
    "    for loader in loaders:\n",
    "        docs.extend(\n",
    "            loader.load(),\n",
    "        )\n",
    "    return docs\n",
    "\n",
    "# Define the metadata extraction function.\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    \n",
    "    metadata[\"urls\"] = record.get(\"urls\")\n",
    "    metadata[\"job_name\"] = record.get(\"job_name\")\n",
    "    metadata[\"company_name\"] = record.get(\"company_name\")\n",
    "    metadata[\"address\"] = record.get(\"address\")\n",
    "    metadata[\"salary\"] = record.get(\"salary\")\n",
    "    metadata[\"remaining\"] = record.get(\"remaining\")\n",
    "    # metadata[\"\"] = record.get(\"Mô tả công việc\")\n",
    "    metadata[\"Yêu cầu ứng viên\"] = record.get(\"Yêu cầu ứng viên\")\n",
    "    metadata[\"Quyền lợi\"] = record.get(\"Quyền lợi\")\n",
    "    metadata[\"Địa điểm làm việc\"] = record.get(\"Địa điểm làm việc\")\n",
    "    metadata[\"Cách thức ứng tuyển\"] = record.get(\"Cách thức ứng tuyển\")\n",
    "    metadata[\"Cấp bậc\"] = record.get(\"Cấp bậc\")\n",
    "    metadata[\"Kinh nghiệm\"] = record.get(\"Kinh nghiệm\")\n",
    "    metadata[\"Số lượng tuyển\"] = record.get(\"Số lượng tuyển\")\n",
    "    metadata[\"Hình thức làm việc\"] = record.get(\"Hình thức làm việc\")\n",
    "    metadata[\"Giới tính\"] = record.get(\"Giới tính\")\n",
    "    \n",
    "    metadata = {key: f'{\" \".join(val) if isinstance(val, list) else val}' for key, val in metadata.items()}\n",
    "\n",
    "    \n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def load_jsonl(\n",
    "    files: Union[str, List[str]] = \"../data/crawl/train_test.jsonl\"\n",
    ") -> List[Document]:\n",
    "    if isinstance(files, str):\n",
    "        loader = JSONLoader(\n",
    "            files,\n",
    "            json_lines=True,\n",
    "            jq_schema='.',\n",
    "            content_key=\"Mô tả công việc\",\n",
    "            text_content=False,\n",
    "            metadata_func=metadata_func\n",
    "        )\n",
    "        return loader.load()\n",
    "\n",
    "    loaders = [\n",
    "        JSONLoader(\n",
    "            file,\n",
    "            json_lines=True,\n",
    "            jq_schema='.messages[]',\n",
    "            content_key=\"content\",\n",
    "            metadata_func=metadata_func\n",
    "        )\n",
    "        for file in files\n",
    "    ]\n",
    "    docs = []\n",
    "    for loader in loaders:\n",
    "        docs.extend(\n",
    "            loader.load(),\n",
    "        )\n",
    "    return docs\n",
    "\n",
    "def create_parent_retriever(\n",
    "    docs: List[Document], embeddings_model: HuggingFaceBgeEmbeddings()\n",
    "):\n",
    "    parent_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\\n\", \"\\n\\n\"],\n",
    "        chunk_size=2000,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    # This text splitter is used to create the child documents\n",
    "    child_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\\n\", \"\\n\\n\"],\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=300,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    # The vectorstore to use to index the child chunks\n",
    "    # vectorstore = Chroma(\n",
    "    #     collection_name=\"split_documents\",\n",
    "    #     embedding_function=embeddings_model,\n",
    "    #     persist_directory=persist_directory,\n",
    "    # ).as_retriever()\n",
    "    \n",
    "    vectorstore = Chroma.from_documents(docs, embeddings_model).as_retriever()\n",
    "    # The storage layer for the parent documents\n",
    "    store = InMemoryStore()\n",
    "    retriever = ParentDocumentRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        child_splitter=child_splitter,\n",
    "        parent_splitter=parent_splitter,\n",
    "        k=10,\n",
    "    )\n",
    "    retriever.add_documents(docs)\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def retrieve_context(query, retriever, reranker_model):\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "    if len(retrieved_docs) == 0:\n",
    "        raise RAGException(\n",
    "            f\"Couldn't retrieve any relevant document with the query `{query}`. Try modifying your question!\"\n",
    "        )\n",
    "    reranked_docs = rerank_docs(\n",
    "        query=query, retrieved_docs=retrieved_docs, reranker_model=reranker_model\n",
    "    )\n",
    "    return reranked_docs\n",
    "\n",
    "\n",
    "def load_embedding_model(\n",
    "    model_name: str = \"BAAI/bge-large-en-v1.5\", device: str = \"cuda\"\n",
    ") -> HuggingFaceBgeEmbeddings:\n",
    "    model_kwargs = {\"device\": device}\n",
    "    encode_kwargs = {\n",
    "        \"normalize_embeddings\": True\n",
    "    }  # set True to compute cosine similarity\n",
    "    embedding_model = HuggingFaceBgeEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "    )\n",
    "    return embedding_model\n",
    "\n",
    "\n",
    "def load_reranker_model(\n",
    "    reranker_model_name: str = \"BAAI/bge-reranker-large\", device: str = \"cuda\"\n",
    ") -> CrossEncoder:\n",
    "    reranker_model = CrossEncoder(\n",
    "        model_name=reranker_model_name, max_length=512, device=device\n",
    "    )\n",
    "    return reranker_model\n",
    "\n",
    "\n",
    "def main(\n",
    "    file: str = \"../data/crawl/train_test.jsonl\",\n",
    "    query: Optional[str] = None,\n",
    "    llm_name=\"mistral\",\n",
    "):\n",
    "    # docs = load_pdf(files=file)\n",
    "    docs = load_jsonl(file)\n",
    "    # print(docs)\n",
    "\n",
    "    embedding_model = load_embedding_model()\n",
    "    retriever = create_parent_retriever(docs, embedding_model)\n",
    "    reranker_model = load_reranker_model()\n",
    "\n",
    "    context = retrieve_context(\n",
    "        query, retriever=retriever, reranker_model=reranker_model\n",
    "    )[0]\n",
    "    print(\"context:\\n\", context, \"\\n\", \"=\" * 50, \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # from jsonargparse import CLI\n",
    "\n",
    "    # CLI(main)\n",
    "    main(query=\"What is the job description for Network Engineer?\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n",
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Data Science/AI Engineer\\n\\nTIEN PHAT BUI\\n\\n0962980173 | buitienphat2462002@gmail.com https://www.linkedin.com/in/tien-phat-bui-ba3421222/ https://github.com/ShynBui/ Go Vap district, Ho Chi Minh city\\n\\nEducation\\n\\nB ACH EL OR OF IN FOR M ATION TECH N OL O G Y(Expected 05/2024) – Ho Chi Minh City Open University – 97 Vo Van Tan, Ward 6, District 3, Ho Chi Minh City GPA: 3.76/4.00 (Highest in Major) Majors: Information Technology Relevant Coursework: Machine Learning, Data Visualization and Communication, Database Systems and SQL, Natural Language Processing, Computer Vision, Deep Learning and Neural Networks\\n\\nSkills\\n\\nPython Libraries and frameworks: pandas, matplotlib, numpy, BeutifulSoup4, re, os, scikit-learn, Hugging Face, transformers, PyTorch, Python-Flask, LangChain, NLTK Big Data Apache Hadoop, PySpark SQL Software: SQLite, MySQL, SQL, PostgreSQL Power Bi Tools: Power Query, Visualize C++ Libraries: STL, Boot C++, Armadillo\\n\\nProjects\\n\\nLEGAL SEAR CH AN D R ES PON SE PLATFOR M – Group competition project\\n\\nNovember 2023 \\n\\n-\\n\\n now\\n\\n● Provides users with quick access to legal information, delivers reliable answers to legal queries, and enables efficient navigation through legal codes and documents\\n\\n● Utilizes ReactJS for front\\n\\n-end development, employs Python Flask for back\\n\\n-end functionality, and enhances its capabilities with complementary integration tools such as CI/CD (Continuous Integration/Continuous Deployment) and Docker for streamlined deployment processes, ensuring efficient and reliable operation\\n\\n● Project management, ensuring a waterfall system development process, data search and preprocessing, training deep learning models based on PhoBERT and LangChain for question\\n\\n-answering tasks for answering legal and training another model to classify \"legal definition\" sentences in legal documents, taking on the role of data designer using MySQL, and visualize the model training process for reporting using matplotlib\\n\\nWinning third prize in the Vietnam Student Informatics Olympiad - Open source competition, the project attracted the jury for its creativity and was considered to have the most potential. The product is open source code that is being invested in, developed and raised capital The article introduces the project: https://vfossa.vn/tin-tuc/code-heroes-nen-tang-tra-cuu-va-giai-dap-phap-luat-ung-dung-sang- tao-cac-mo-hinh-ngon-ngu-lon-698.html\\n\\nAN AUTOM ATICALLY AN S WER IN G EN GLISH AN D VIETN AM ESE R EA D IN G COM PR EH EN SION S YSTEM – Personal June 2021 - January 2022 Project\\n\\n● Assisting users in automatically answering Vietnamese or English reading comprehension questions by providing the system with\\n\\na passage of information (provided by the user), from which the system extracts information to answer the user\\'s queries ● Using Hugging Face as a model repository, we employ a BERT-based model for answering English reading comprehension\\n\\nUsing Hugging Face as a model repository, we employ a BERT-based model for answering English reading comprehension questions and a PhoBERT-based model for answering Vietnamese reading comprehension questions. Gradio is utilized to design an interactive interface for user interaction\\n\\nquestions and a PhoBERT-based model for answering Vietnamese reading comprehension questions. Gradio is utilized to design an interactive interface for user interaction Taking on the role of data preparation, I utilize available datasets and scrape data from the web using BeautifulSoup4. Data processing is performed using Python, while model fine-tuning is carried out through the use of Transformers and PyTorch. Model evaluation and visualization of evaluation results are conducted using matplotlib The English question-answering model achieved a significant 78% accuracy when applied to multiple-choice questions. The Vietnamese question-answering model also achieved an exact match accuracy of 72%\\n\\n\\n\\n\\n\\nOTH ER PR OJECT\\n\\nOnline library ● ● Website for book sales management ● Vietnamese toxic comment classification\\n\\nAchievements and Awards\\n\\nTH E 2022 ICPC ASI A H O CH I M INH CITY – HCMUTE – Ho Chi Minh City\\n\\nDecember 2022\\n\\n● Ranked among the top 260 / 470 teams in the ICPC Asia Regionals, showcasing problem\\n\\n-solving skills and algorithmic proficiency.\\n\\nTH E 2023 ICPC ASI A H UE CITY – Hue Uni ver si ty Of Sci ence – Hue City\\n\\nDecember 2023\\n\\nThird prize in the Vietnam Student Informatics Olympiad - Open source competition. Solution building project \"Developing an application to support searching and asking questions about legal knowledge based on the Legal Code and Legal Document Database\", applying and refining LLMs models to training chat-bot to answer legal questions\\n\\n\\n\\nTH E 24H PR OGR AMM IN G – Ho Chi Mi nh Ci ty Open Uni ver si ty – Ho Chi Mi nh Ci ty\\n\\nApril 2023\\n\\n● Champion of the 24h Programming Contest. Build an application that applies algorithms to suggest user habits using machine learning and visualize daily activities and consumption habits using matplotlib, helping users manage time and money\\n\\nWork Experience\\n\\nDATA AN AL YST – I SD CORP – Tan Binh, HCM City\\n\\nJune 2023 \\n\\n-\\n\\n August 2023\\n\\n● Created interactive dashboards and reports using Power BI, matplotlib to visualize and communicate data\\n\\n-driven insights to stakeholders\\n\\n● Conducted data cleaning, preprocessing, and feature engineering to ensure data quality and accuracy. Utilized statistical analysis techniques and machine learning algorithms to develop predictive models for business forecasting\\n\\n● Develop charts in the CRM system to evaluate the business performance of the enterprise. Proficient in using SQL techniques to query data, proficiently applying matplotlib, Power BI and scikit\\n\\n-learn machine learning techniques to enhance customers\\' decision making process\\n\\nOptimized data processing workflows, resulting in a 10% reduction in data processing time ● Developed a comprehensive customer segmentation model that improved targeted marketing campaigns and increased\\n\\ncustomer engagement\\n\\nCertification\\n\\nDATA SCIEN C E AN D M ACH IN E L EAR N IN G CER T IFICATES\\n\\nHo Chi Minh City University of Natural Sciences Informatics Center ● Certification Date: 2023 ● Description: Helps master Python programming, use various tools and libraries for data science and machine learning engineer, apply data preprocessing techniques, implement machine learning algorithms to solve real problems economics, and work with Big Data technologies such as Spark and PySpark\\n\\nICPC PAR TIC IPATI ON C ER TIFICAT E\\n\\n● Certification Date: October 2022 \\n\\n● Description: The ICPC (International Collegiate Programming Contest) certificate is awarded to participants who have competed in the ICPC regional or world finals competitions. It serves as a recognition of their accomplishment and participation in this prestigious programming contest', metadata={'source': '../data/cv/Bui Tien Phat resume (1).pdf'})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_pdf(\n",
    "    files: Union[str, List[str]] = \"../../data/cv/Bui Tien Phat resume (1).pdf\"\n",
    ") -> List[Document]:\n",
    "    if isinstance(files, str):\n",
    "        loader = UnstructuredFileLoader(\n",
    "            files,\n",
    "            post_processors=[clean_extra_whitespace, group_broken_paragraphs],\n",
    "        )\n",
    "        return loader.load()\n",
    "\n",
    "    loaders = [\n",
    "        UnstructuredFileLoader(\n",
    "            file,\n",
    "            post_processors=[clean_extra_whitespace, group_broken_paragraphs],\n",
    "        )\n",
    "        for file in files\n",
    "    ]\n",
    "    docs = []\n",
    "    for loader in loaders:\n",
    "        docs.extend(\n",
    "            loader.load(),\n",
    "        )\n",
    "    return docs\n",
    "\n",
    "load_pdf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
